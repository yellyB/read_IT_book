272(247p) ~

웹 로봇 = 크롤러, 스파이더, 웜, 봇 …

1. 크롤러와 크롤링
- 루트 집합: 크롤러가 방문을 시작하는 URL의 초기 집합
- 크롤러가 중복 방문을 피하기 위해 사용하는 기법
        - 트리와 해시 테이블
        - 느슨한 존재 비트맵: 존재 비트 배열 자료구조 사용
        - 체크포인트: 갑작스레 중단될 경우 대비, 방문한 URL목록이 디스크에 저장되었나 확인
        - 파티셔닝: 각 로봇에 URL의 특정 부분을 할당
- 크롤러가 곤란에 빠지는 경우들
        - URL이 별칭을 가지고 있으면 URL은 서로 다르지만 같은 리소스인 문제
                ⇒ 정규화로 해결하려 했지만 다른 형식의 별칭은 대응 불가
        - 파일 시스템 링크 순환: 끝없이 깊어지는 디렉터리 계층
        - 동적 가상 웹 공간: 예)계속 다음달 링크를 주는 프로그램⇒로봇은 무한히 다음달 달력 요청
- 루프와 중복 피하기
        - URL 정규화
        - 너비 우선 크롤링: 순환을 건드리더라도 다른 웹 사이트에서 페이지 받아올 수 있음
        - 스로틀링: 일정 시간 가져올 수 있는 페이지 숫자 제한
        - URL 크기 제한: 일정 길이 넘는 URL제한하는 방법. 단점은 가져오지 못하는 콘텐츠도 있단 것
        - URL/사이트 블랙리스트
        - 패턴 발견
        - 콘텐츠 지문: 페이지 콘텐츠에서 몇 바이트 얻어와 체크섬을 계산. 다른 페이지인데도 체크섬 함수 결과 같지 않도록 잘 설계해야함
        - 사람의 모니터링
2. 로봇의 HTTP
- 요청 헤더: 신원 식별 헤더
        - User-Agent : 로봇의 이름
        - From : 로봇 관리자의 이메일 주소
        - Accept : 어떤 미디어 타입을 보내도 되는지(로봇이 관심 있는 유형의 콘텐츠)
        - Referer : 현재의 요청 URL 포함한 문서의 URL
- 가상 호스팅
        ⇒ 두 개의 사이트를 운영하는 서버에 요청을 보낼 때 Host 헤더가 없다면
             www.B.com에 요청을 보냈는데 기본으로 설정된 www.A.com에 대한 응답을 얻게 될 수도
- 조건부 요청: 시간이나 엔터티 태그 비교해서 마지막 버전 이후 업데이트 된 것이 있나 알아보는 요청
       - 대다수 로봇은 GET 메서드만, but 조건부 요청 등 서버와 상호작용을 하려고 하는 로봇들도 있음
       - 사이트 관리자는 로봇이 방문했다가 콘텐츠 못가져가는일 없도록 전략 세워야 함

1. 부적절하게 동작하는 로봇들
- 폭주하는 로봇: 웹 서버에 극심한 부하
- 오래된 URL: 존재하지 않는 문서에 접근해서 에러 로그 남기는 등
- 길고 잘못된 URL: 웹 서버 처리 능력에 영향 줌 + 접근 로그 어지럽힘 + 고장 가능성도 있음
- 호기심이 지나친 로봇: 사적인 데이터에 접근
- 동적 게이트웨이 접근
2. 로봇 차단하기
- robots.txt : 로봇 동작을 더 잘 제어할 수 있는 매커니즘 제공하는 기법.
  : 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보 담겨있음
  : 웹사이트 방문하기 전에 그 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일 가져와서 처리해야함
- HTML 로봇 제어 META 태그: 직접 HTML 문서에서 제어하는 방법
        <META NAME=”ROBOTS” CONTENT=”NOINDEX”> : 이 페이지 무시해줘
        <META NAME=”ROBOTS” CONTENT=”NOFOLLOW”> : 내가 링크한 페이지를 크롤링하지 마
3. 로봇 에티켓
- http://www.robotstxt.org/wc/guidelines/html
4. 검색엔진
- 웹 로봇을 가장 광범위하게 사용함. 웹 크롤러는 검색엔진에게 먹이주듯 문서들을 가져다줌
- ‘full-text indexes’ : 현대적인 검색엔진 아키텍처. 로컬 데이터베이스 생성해서 웹의 모든 문서에대한 카탈로그처럼 동작
- 관련도 랭킹: 검색엔진은 결과에 순위를 매기는 알고리즘 사용. 가장 관련 깊은 문서가 검색되도록
- 스푸핑: 웹 마스터가 관련 없는 키워드 나열한 가짜 페이지 만들거나 검색엔진 알고리즘 속이는 등
        ⇒ 검색엔진과 로봇 구현자는 이런 속임수 잘 잡아내기 위해 끊임없이 알고리즘 수정해야함
